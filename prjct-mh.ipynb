{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# *Library*\nImport *library* yang dibutuhkan.","metadata":{}},{"cell_type":"code","source":"!pip3 install num2words # Install library untuk mengubah angka menjadi kata.\n!pip install Sastrawi # Karena wordnet tidak bisa digunakan maka di sini pakai Sastrawi.","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:00:00.932925Z","iopub.execute_input":"2023-04-26T17:00:00.933340Z","iopub.status.idle":"2023-04-26T17:00:24.125521Z","shell.execute_reply.started":"2023-04-26T17:00:00.933303Z","shell.execute_reply":"2023-04-26T17:00:24.124395Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Library untuk membaca file .csv.\nimport pandas as pd\n# Library untuk visualisasi.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Library untuk memproses teks.\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport re\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom num2words import num2words\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n# Library untuk membangun model.\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Bidirectional\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import regularizers\nimport numpy as np\n# Library untuk memisahkan dataset.\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\n# Library untuk evaluasi model.\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:00:24.128807Z","iopub.execute_input":"2023-04-26T17:00:24.129875Z","iopub.status.idle":"2023-04-26T17:00:24.145279Z","shell.execute_reply.started":"2023-04-26T17:00:24.129809Z","shell.execute_reply":"2023-04-26T17:00:24.144062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('punkt') # Untuk tokenizer kata.\nnltk.download('stopwords')# Untuk menghapus stopwords.\n# nltk.download('wordnet') # Ada masalah ketika menggunakan wordnet.","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:00:24.146776Z","iopub.execute_input":"2023-04-26T17:00:24.147215Z","iopub.status.idle":"2023-04-26T17:00:24.285575Z","shell.execute_reply.started":"2023-04-26T17:00:24.147174Z","shell.execute_reply":"2023-04-26T17:00:24.284430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Membaca Dataset\nMelakukan visualisasi dataset yang dimiliki.","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/data-mh-id/data_mh.csv\", delimiter=\",\") # Sesuaikan `delimiter` berdasarkan bahasa yang digunakan dalma .csv.\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:00:24.288344Z","iopub.execute_input":"2023-04-26T17:00:24.288952Z","iopub.status.idle":"2023-04-26T17:00:24.407969Z","shell.execute_reply.started":"2023-04-26T17:00:24.288908Z","shell.execute_reply":"2023-04-26T17:00:24.406706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualisasi label, usahakan label yang dimiliki seimbang.","metadata":{}},{"cell_type":"code","source":"df[\"is_depression\"].value_counts().plot(kind=\"bar\",figsize=(5,3))","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:00:24.409217Z","iopub.execute_input":"2023-04-26T17:00:24.410174Z","iopub.status.idle":"2023-04-26T17:00:24.585653Z","shell.execute_reply.started":"2023-04-26T17:00:24.410129Z","shell.execute_reply":"2023-04-26T17:00:24.584667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cek apakah terdapat null value atau tidak.\nprint(df.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:00:24.586812Z","iopub.execute_input":"2023-04-26T17:00:24.587431Z","iopub.status.idle":"2023-04-26T17:00:24.598309Z","shell.execute_reply.started":"2023-04-26T17:00:24.587389Z","shell.execute_reply":"2023-04-26T17:00:24.596958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cek tipe data kolom.\nprint(df.info())","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:00:24.600045Z","iopub.execute_input":"2023-04-26T17:00:24.600441Z","iopub.status.idle":"2023-04-26T17:00:24.616692Z","shell.execute_reply.started":"2023-04-26T17:00:24.600397Z","shell.execute_reply":"2023-04-26T17:00:24.614964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Memproses Data Teks\nProses data teks seperti menghapus `stop_words`, menjadikannya kata dasar, mengganti angka ke dalam teks, dll.","metadata":{}},{"cell_type":"code","source":"factory = StemmerFactory()\nstemmer = factory.create_stemmer()\nstop_words = set(stopwords.words('indonesian'))\n\n# def translate_text(text):\n#     url = \"https://translate.googleapis.com/translate_a/single?client=gtx&sl=en&tl=id&dt=t&q={}\".format(text)\n#     response = requests.get(url)\n#     result = response.json()[0][0][0]\n#     return result\n\ndef preprocess_text(text):\n#     text = translate_text(text) # Kemudian di sini kita coba translate\n    text = word_tokenize(text.lower()) # Tokenize text_id ke dalam token/kata\n    text = [t for t in text if t not in stop_words] # Hapus stop_words\n    text = [stemmer.stem(t) for t in text] # Menjadikan kata dasar\n    text = [t if not t.isdigit() else num2words(int(t)) for t in text] # Mengganti angka ke teks\n#     text = emoji.demojize(text) # ubah emoji ke dalam bentuk teks\n#     text = re.sub(r':[a-z_]+:', lambda m: ' '.join(m.group(0).replace(':', '').split('_')), text) # Regex misal emoji: ðŸŽ‰ bakal diubah jadi \"party popper\".\n    text = ' '.join(text) # Gabungkan ke dalam teks kembali\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:00:24.618513Z","iopub.execute_input":"2023-04-26T17:00:24.618920Z","iopub.status.idle":"2023-04-26T17:00:24.645443Z","shell.execute_reply.started":"2023-04-26T17:00:24.618884Z","shell.execute_reply":"2023-04-26T17:00:24.644111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[pd.notnull(df['text_id'])] # hapus semua baris dengan nilai NaN di kolom `text`\ndf = df[df['text_id'].apply(lambda x: isinstance(x, str))] # hapus semua baris di mana `text` bukan string\ndf['nlp_text'] = df['text_id'].apply(lambda x: preprocess_text(x))","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:00:24.648668Z","iopub.execute_input":"2023-04-26T17:00:24.649054Z","iopub.status.idle":"2023-04-26T17:27:11.878953Z","shell.execute_reply.started":"2023-04-26T17:00:24.649018Z","shell.execute_reply":"2023-04-26T17:27:11.877558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:27:11.883296Z","iopub.execute_input":"2023-04-26T17:27:11.884260Z","iopub.status.idle":"2023-04-26T17:27:11.903554Z","shell.execute_reply.started":"2023-04-26T17:27:11.884212Z","shell.execute_reply":"2023-04-26T17:27:11.902276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hitung Ukuran *Vocabulary*\nMemasukan kata unik ke dalam *se* `words`, kemudian untuk menghitung ukuran *vocabulary*.","metadata":{}},{"cell_type":"code","source":"words = set(word for sentence in df[\"nlp_text\"] for word in sentence.split())\nvocab_size = len(words)\nprint(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:32:12.256974Z","iopub.execute_input":"2023-04-26T17:32:12.257399Z","iopub.status.idle":"2023-04-26T17:32:12.303893Z","shell.execute_reply.started":"2023-04-26T17:32:12.257356Z","shell.execute_reply":"2023-04-26T17:32:12.302539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# *One Hot Encoding* untuk Setiap Kalimat\n*One-hot encode* teks ke dalam daftar indeks kata berukuran n.","metadata":{}},{"cell_type":"code","source":"one_hot_representation = [one_hot(words,vocab_size) for words in df[\"nlp_text\"]] # Input teksnya `words` di setiap `df[\"nlp_text\"]` dengan ukuran `vocab_size`\n# one_hot_representation","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:32:13.851837Z","iopub.execute_input":"2023-04-26T17:32:13.852980Z","iopub.status.idle":"2023-04-26T17:32:14.039773Z","shell.execute_reply.started":"2023-04-26T17:32:13.852932Z","shell.execute_reply":"2023-04-26T17:32:14.038435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cari panjang maksimum untuk dimasukan ke dalam word embeddings.","metadata":{}},{"cell_type":"code","source":"max_len = 0\nfor elem in one_hot_representation:\n    if len(elem) > max_len:\n        max_len = len(elem)\nmax_len","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:32:15.468995Z","iopub.execute_input":"2023-04-26T17:32:15.469448Z","iopub.status.idle":"2023-04-26T17:32:15.479420Z","shell.execute_reply.started":"2023-04-26T17:32:15.469404Z","shell.execute_reply":"2023-04-26T17:32:15.478031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Mengonversi *One-hot Encodings* ke *Word Embeddings*\nMeskipun *one-hot encodings* adalah teknik yang berguna untuk merepresentasikan kata-kata sebagai vektor numerik di NLP, teknik ini memiliki beberapa keterbatasan. Salah satunya adalah vektor *one-hot* berdimensi tinggi dan *sparse*, yang berarti bahwa sebagian besar elemen vektor adalah 0, yang dapat mempersulit algoritma pembelajaran mesin untuk mengekstraksi pola yang bermakna dari data.\nUntuk mengatasi keterbatasan ini, kita dapat mengonversi vektor *one-hot* menjadi *word embeddings*, yaitu vektor yang sifatnya *dense* berdimensi rendah yang merepresentasikan kata dalam ruang vektor kontinu.","metadata":{}},{"cell_type":"markdown","source":"Ketika panjang sequences kurang dari max_length maka pad_sequences akan menambahkan padding supaya jumlahnya sama dengan max_length. padding = 'post' dibuat untuk menambahkan padding di akhir, jika padding = 'pre' maka padding akan ditambahkan di awal sequences.\n\nKemudian apabila panjang sequences lebih dari max_length maka akan ditruncate. padding = 'post' dibuat untuk truncate di akhir, jika padding = 'pre' maka dibuat untuk truncate di awal.","metadata":{}},{"cell_type":"code","source":"# embedded_docs = pad_sequences(one_hot_representation, padding='pre', maxlen = max_len)\nembedded_docs = pad_sequences(one_hot_representation, padding = 'post',truncating = 'post', maxlen = max_len)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:32:17.669621Z","iopub.execute_input":"2023-04-26T17:32:17.670073Z","iopub.status.idle":"2023-04-26T17:32:17.730498Z","shell.execute_reply.started":"2023-04-26T17:32:17.670036Z","shell.execute_reply":"2023-04-26T17:32:17.729365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Model 1\n# embedding_vector_features = max_len * 2\n# model = Sequential()\n# model.add(Embedding(14233, embedding_vector_features, input_length=max_len))\n# model.add((LSTM(100)))\n# model.add(Dense(1,activation = 'sigmoid'))\n# model.compile(loss='binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n# print(model.summary())","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:34:05.461493Z","iopub.execute_input":"2023-04-26T17:34:05.462997Z","iopub.status.idle":"2023-04-26T17:34:05.809264Z","shell.execute_reply.started":"2023-04-26T17:34:05.462937Z","shell.execute_reply":"2023-04-26T17:34:05.807068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Model 2\nembedding_dim = 100\nmodel = Sequential()\nmodel.add(Embedding(input_dim=14233, output_dim=embedding_dim, input_length=max_len))\nmodel.add(Bidirectional(LSTM(units=128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\nmodel.add(Bidirectional(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2)))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = df[\"is_depression\"]","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:34:07.727616Z","iopub.execute_input":"2023-04-26T17:34:07.728089Z","iopub.status.idle":"2023-04-26T17:34:07.733877Z","shell.execute_reply.started":"2023-04-26T17:34:07.728045Z","shell.execute_reply":"2023-04-26T17:34:07.732490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Jumlah sample `y` dengan `embedded_docs` harus sama.","metadata":{}},{"cell_type":"code","source":"print(f\"Shape of y is: {y.shape}\")\nprint(f\"Shape of embedded document is: {embedded_docs.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:34:08.592464Z","iopub.execute_input":"2023-04-26T17:34:08.593245Z","iopub.status.idle":"2023-04-26T17:34:08.599282Z","shell.execute_reply.started":"2023-04-26T17:34:08.593198Z","shell.execute_reply":"2023-04-26T17:34:08.597823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Test Split","metadata":{}},{"cell_type":"markdown","source":"Di sini kita split `test_size` sebesar 0.1 dikarenakan jumlah datasetnya berukuran besar. Kemudian menggunakan `stratifikasi` dalam fungsi `train_test_split()` memastikan bahwa `training set` dan `test set` memiliki distribusi sampel yang proporsional dari setiap label kelas dalam variabel target, yang dapat membantu jika set data tidak seimbang.","metadata":{}},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(embedded_docs, y, test_size = 0.1, random_state = 42, stratify = y)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:34:09.220581Z","iopub.execute_input":"2023-04-26T17:34:09.221006Z","iopub.status.idle":"2023-04-26T17:34:09.273912Z","shell.execute_reply.started":"2023-04-26T17:34:09.220970Z","shell.execute_reply":"2023-04-26T17:34:09.272596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Shape of X train is: {X_train.shape}\")\nprint(f\"Shape of y train is: {y_train.shape}\")\nprint(f\"Shape of X test is: {X_test.shape}\")\nprint(f\"Shape of y test is: {y_test.shape}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:34:09.792455Z","iopub.execute_input":"2023-04-26T17:34:09.793567Z","iopub.status.idle":"2023-04-26T17:34:09.800329Z","shell.execute_reply.started":"2023-04-26T17:34:09.793514Z","shell.execute_reply":"2023-04-26T17:34:09.798881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Pastikan *X train* dengan *y train* punya jumlah sampel yang sama dan *X test* dengan *y test* punya jumlah sampel yang sama.\n\nSelain itu pastikan kolom *X train* dengan *X test* berjumlah sama dan *y train* dengan *y test* berjumlah sama.\n\n**Contoh: (jumlah sampel, jumlah kolom)**","metadata":{}},{"cell_type":"code","source":"history = model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=3,batch_size=16) # Kayanya epoch cukup di 3 aja\n\n#### Kalau ga cukup coba pakai chunking\n# chunk_size = 1000  # set the size of each chunk\n# # shuffle the data\n# X_train, Y_train = shuffle(X_train, Y_train)\n# # loop through the data in chunks\n# for i in range(0, len(X_train), chunk_size):\n#     X_chunk = X_train[i:i+chunk_size]\n#     Y_chunk = Y_train[i:i+chunk_size]  \n#     # train the model on the current chunk\n#     model.fit(X_chunk, Y_chunk, epochs=1, batch_size=32, validation_data=(X_test, Y_test))","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:34:10.254464Z","iopub.execute_input":"2023-04-26T17:34:10.255538Z","iopub.status.idle":"2023-04-26T17:56:51.787254Z","shell.execute_reply.started":"2023-04-26T17:34:10.255489Z","shell.execute_reply":"2023-04-26T17:56:51.786172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:27:15.959300Z","iopub.status.idle":"2023-04-26T17:27:15.959745Z","shell.execute_reply.started":"2023-04-26T17:27:15.959522Z","shell.execute_reply":"2023-04-26T17:27:15.959544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Probabilitas lebih besar dari atau sama dengan 0,5 akan diberikan ke kelas positif.","metadata":{}},{"cell_type":"code","source":"y_pred = (y_pred >= 0.5).astype(\"int\")","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:27:15.960844Z","iopub.status.idle":"2023-04-26T17:27:15.961264Z","shell.execute_reply.started":"2023-04-26T17:27:15.961042Z","shell.execute_reply":"2023-04-26T17:27:15.961063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test,y_pred))\nprint(confusion_matrix(y_test,y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:27:15.962280Z","iopub.status.idle":"2023-04-26T17:27:15.962686Z","shell.execute_reply.started":"2023-04-26T17:27:15.962465Z","shell.execute_reply":"2023-04-26T17:27:15.962485Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = y_test.reset_index(drop = True).values # Reset index `y_test` supaya dari 0 lagi sebelum dikonversi ke dalam array","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:27:15.964616Z","iopub.status.idle":"2023-04-26T17:27:15.965330Z","shell.execute_reply.started":"2023-04-26T17:27:15.965117Z","shell.execute_reply":"2023-04-26T17:27:15.965142Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model.save('model.h5')","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:27:15.967016Z","iopub.status.idle":"2023-04-26T17:27:15.968189Z","shell.execute_reply.started":"2023-04-26T17:27:15.967934Z","shell.execute_reply":"2023-04-26T17:27:15.967968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#### Belum selesai (sebentar aku beresin)\n# from keras.models import load_model\n\n# model = load_model('model.h5')\n\n# # Take user input from the command line\n# text = input(\"Enter your text: \")\n\n# # Tokenize and preprocess the text (use the same preprocessing steps as in training)\n# preprocessed_text = preprocess(text)\n\n# # Convert preprocessed text to a numpy array\n# X = np.array([preprocessed_text])\n\n# # Make predictions using your trained model\n# y_pred_prob = model.predict(X)\n# y_pred = np.argmax(y_pred_prob, axis=1)\n\n# # Print the predicted label (0 or 1)\n# print(\"Predicted label: {}\".format(y_pred[0]))","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:27:15.969852Z","iopub.status.idle":"2023-04-26T17:27:15.970665Z","shell.execute_reply.started":"2023-04-26T17:27:15.970423Z","shell.execute_reply":"2023-04-26T17:27:15.970449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(len(y_test)):\n    print(\"True label: {}, Predicted label: {}\".format(y_test[i], y_pred[i]))","metadata":{"execution":{"iopub.status.busy":"2023-04-26T17:27:15.971605Z","iopub.status.idle":"2023-04-26T17:27:15.972036Z","shell.execute_reply.started":"2023-04-26T17:27:15.971824Z","shell.execute_reply":"2023-04-26T17:27:15.971845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"h = history.history\nprint(h.keys())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot fungsi loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plot akurasi\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='lower right')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}